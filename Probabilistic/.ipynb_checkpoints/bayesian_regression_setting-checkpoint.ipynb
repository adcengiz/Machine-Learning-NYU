{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib as matlib\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import support_code\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayesian Linear Regression - Implementation\n",
    "\n",
    "### 1 - Implement Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood_func(w, X, y_train, likelihood_var):\n",
    "    '''\n",
    "    Implement likelihood_func. This function returns the data likelihood\n",
    "    given f(y_train | X; w) ~ Normal(Xw, likelihood_var).\n",
    "\n",
    "    Args:\n",
    "        w: Weights\n",
    "        X: Training design matrix with first col all ones (np.matrix)\n",
    "        y_train: Training response vector (np.matrix)\n",
    "        likelihood_var: likelihood variance\n",
    "\n",
    "    Returns:\n",
    "        likelihood: Data likelihood (float)\n",
    "    '''\n",
    "    #TO DO\n",
    "    # w = np.matrix(w).reshape(len(w), 1)\n",
    "    data_likelihood = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        like_i = ((1/(np.sqrt(likelihood_var))*(np.sqrt(2*(np.pi))))*(np.exp(-(np.square(y_train[i]-np.dot(X[i], w)))/(2*likelihood_var))))\n",
    "        data_likelihood.append(like_i)\n",
    "        \n",
    "    likelihood = np.product(data_likelihood)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior_params(X, y_train, prior, likelihood_var = 0.2**2):\n",
    "    '''\n",
    "    Implement get_posterior_params. This function returns the posterior\n",
    "    mean vector \\mu_p and posterior covariance matrix \\Sigma_p for\n",
    "    Bayesian regression (normal likelihood and prior).\n",
    "\n",
    "    Note support_code.make_plots takes this completed function as an argument.\n",
    "\n",
    "    Args:\n",
    "        X: Training design matrix with first col all ones (np.matrix)\n",
    "        y_train: Training response vector (np.matrix)\n",
    "        prior: Prior parameters; dict with 'mean' (prior mean np.matrix)\n",
    "               and 'var' (prior covariance np.matrix)\n",
    "        likelihood_var: likelihood variance- default (0.2**2) per the lecture slides\n",
    "\n",
    "    Returns:\n",
    "        post_mean: Posterior mean (np.matrix)\n",
    "        post_var: Posterior mean (np.matrix)\n",
    "    '''\n",
    "\n",
    "    # TO DO\n",
    "    # e.g.\n",
    "    # prior = {\"mean\": np.matrix(np.zeros(len(y_train))) ,\"var\":(0.5*(np.identity(len(X.shape[1]))))}\n",
    "    \n",
    "    post_mean = np.dot(np.dot\\\n",
    "                       (np.matrix.getI(np.dot(np.transpose(X), X)\\\n",
    "                                       + likelihood_var*(prior[\"var\"])),\\\n",
    "                        np.transpose(X)), y_train)\n",
    "    post_var = np.matrix.getI((((np.sqrt(likelihood_var))**(-2))*(np.dot(np.transpose(X),X))) + np.matrix.getI(prior[\"var\"]))\n",
    "    return post_mean, post_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictive_params(X_new, post_mean, post_var, likelihood_var = 0.2**2):\n",
    "    '''\n",
    "    Implement get_predictive_params. This function returns the predictive\n",
    "    distribution parameters (mean and variance) given the posterior mean\n",
    "    and covariance matrix (returned from get_posterior_params) and the\n",
    "    likelihood variance (default value from lecture).\n",
    "\n",
    "    Args:\n",
    "        X_new: New observation (np.matrix object)\n",
    "        post_mean, post_var: Returned from get_posterior_params\n",
    "        likelihood_var: likelihood variance (0.2**2) per the lecture slides\n",
    "\n",
    "    Returns:\n",
    "        - pred_mean: Mean of predictive distribution\n",
    "        - pred_var: Variance of predictive distribution\n",
    "    '''\n",
    "\n",
    "    # TO DO\n",
    "    # post_mean = get_posterior_params(X, y_train, prior, likelihood_var = 0.2**2)[0]\n",
    "    # post_var = get_posterior_params(X, y_train, prior, likelihood_var = 0.2**2)[1]\n",
    "    \n",
    "    pred_mean = np.dot(np.transpose(post_mean), X_new)\n",
    "    pred_var = np.dot(np.dot(np.transpose(X_new), post_var), X_new) + likelihood_var\n",
    "    \n",
    "    return pred_mean, pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(46134)\n",
    "actual_weights = np.matrix([[0.3], [0.5]])\n",
    "data_size = 40\n",
    "noise = {\"mean\":0, \"var\":0.2 ** 2}\n",
    "likelihood_var = noise[\"var\"]\n",
    "xtrain, ytrain = support_code.generate_data(data_size, noise, actual_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
