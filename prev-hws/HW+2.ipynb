{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Data Set and Programming Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the problem from disk into memory with load_problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating target function, featurizer, and train/validation/test splits for the Lasso homework.\n",
    "\n",
    "Author: David S. Rosenberg <david.davidr@gmail.com>\n",
    "License: Creative Commons Attribution 4.0 International License\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def step_fn_generator(stepLoc=0):\n",
    "    def f(x):\n",
    "        ret = np.zeros(len(x))\n",
    "        ret[x >= stepLoc] = 1\n",
    "        return ret\n",
    "    return f\n",
    "\n",
    "def linear_comb_generator(fns, coefs):\n",
    "    def f(x):\n",
    "        return sum(fns[i](x) * coefs[i] for i in range(len(fns)))\n",
    "    return f\n",
    "\n",
    "def get_target_and_featurizer(num_basis_fns = 100, num_nonzero = 10, coefs_true=None):\n",
    "    if coefs_true is not None:\n",
    "        num_basis_fns = len(coefs_true)\n",
    "    else:\n",
    "        nonzero_indices = np.random.choice(num_basis_fns, num_nonzero)\n",
    "        coefs_true = np.zeros(num_basis_fns)\n",
    "        coefs_true[nonzero_indices] = np.random.randn(num_nonzero)\n",
    "\n",
    "    all_basis_fns = [step_fn_generator(stepLoc=s)\n",
    "                     for s in np.linspace(0, 1, num_basis_fns, endpoint=False)]\n",
    "\n",
    "    # Construct target function (the Bayes prediction function)\n",
    "    target_fn = linear_comb_generator(all_basis_fns, coefs_true)\n",
    "\n",
    "    def featurize(x):\n",
    "        n = len(x)\n",
    "        # Featurize input values in [0,1]\n",
    "        X_ftrs = np.empty((n, num_basis_fns))\n",
    "        for ftr_num in range(num_basis_fns):\n",
    "            X_ftrs[:, ftr_num] = all_basis_fns[ftr_num](x)\n",
    "        return X_ftrs\n",
    "\n",
    "    return target_fn, coefs_true, featurize\n",
    "\n",
    "def generate_data(target_fn, n=1000, noise_scale=.25, tdof=6):\n",
    "    # Construct dataset\n",
    "    x = np.sort(np.random.rand(n)) #chooses uniformly from [0,1)\n",
    "    y_target = target_fn(x)\n",
    "    y = y_target + noise_scale * np.random.standard_t(tdof,n)\n",
    "    return x, y\n",
    "\n",
    "def get_data_splits(x, y, test_frac=.2):\n",
    "    n = len(y)\n",
    "    shuffled_indices = np.random.permutation(n)\n",
    "    n_test = int(n * test_frac)\n",
    "    n_train = n - n_test\n",
    "    indices_test = shuffled_indices[:n_test]\n",
    "    indices_train = shuffled_indices[n_test:]\n",
    "    y_train = y[indices_train]\n",
    "    x_train = x[indices_train]\n",
    "    y_test = y[indices_test]\n",
    "    x_test = x[indices_test]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def generate_problem(n=200, num_basis_fns=400, num_nonzero=10,\\\n",
    "                     noise_scale=.25, tdof=6, test_frac=.2, \\\n",
    "                     write_problem=False, file_name=\"lasso_data.pickle\"):\n",
    "    target_fn, coefs_true, featurize = \\\n",
    "    get_target_and_featurizer(num_basis_fns, num_nonzero)\n",
    "    x, y = generate_data(target_fn, n, noise_scale, tdof)\n",
    "    x_train, y_train, x_test, y_test = get_data_splits(x, y, test_frac)\n",
    "\n",
    "    if write_problem:\n",
    "        print (\"Saving problem to disk.\")\n",
    "        data = {\"coefs_true\":coefs_true, \"x_train\":x_train,\n",
    "                \"y_train\":np.copy(y_train), \"x_test\":x_test, \"y_test\":y_test}\n",
    "        with open(file_name, 'wb') as outfile:\n",
    "            pickle.dump(data, outfile, protocol=2)\n",
    "    return x_train, y_train, x_test, y_test, target_fn, coefs_true, featurize\n",
    "\n",
    "def reconstitute_problem(coefs_true, x_train, y_train, x_test, y_test):\n",
    "    target_fn, coefs_true, featurize = get_target_and_featurizer(coefs_true=coefs_true)\n",
    "    return x_train, y_train, x_test, y_test, target_fn, coefs_true, featurize\n",
    "\n",
    "def load_problem(file_name):\n",
    "    f_myfile = open(file_name, 'rb')\n",
    "    data = pickle.load(f_myfile)\n",
    "    f_myfile.close()\n",
    "    return reconstitute_problem(data[\"coefs_true\"], data[\"x_train\"], data[\"y_train\"],\n",
    "                                data[\"x_test\"], data[\"y_test\"])\n",
    "def main():\n",
    "    lasso_data_fname = \"lasso_data.pickle\"\n",
    "    LOAD_PROBLEM=False\n",
    "    GENERATE_PROBLEM=True\n",
    "    WRITE_PROBLEM=False\n",
    "    if GENERATE_PROBLEM:\n",
    "        n=1000\n",
    "        test_frac=.9\n",
    "        num_basis_fns=400\n",
    "        num_nonzero=10\n",
    "        noise_scale=.25 # scale factor on noise\n",
    "        tdof = 6 # degrees of freedom of t-distribution generating noise\n",
    "        x_train, y_train, x_val, y_val, target_fn, coefs_true, \\\n",
    "        featurize = generate_problem(n=n, \\\n",
    "                                     num_basis_fns=num_basis_fns, \\\n",
    "                                     num_nonzero=num_nonzero, \\\n",
    "                                     noise_scale=noise_scale, \\\n",
    "                                     test_frac=test_frac, \\\n",
    "                                     write_problem=WRITE_PROBLEM, \\\n",
    "                                     file_name=lasso_data_fname)\n",
    "\n",
    "    if LOAD_PROBLEM:\n",
    "        x_train, y_train, x_val, y_val, target_fn, \\\n",
    "        coefs_true, featurize = load_problem(lasso_data_fname)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.scatter(x_train, y_train, s=3, color='b', label='Training data')\n",
    "    x = np.arange(0,1,.001)\n",
    "    ax.plot(x, target_fn(x), 'r', label='Target function (i.e. Bayes prediction function)')\n",
    "    legend = ax.legend(loc='upper center', shadow=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'setup_problem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc823f2d7852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msetup_problem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'setup_problem'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ridge regression using scipy's minimize function and demonstrating the use of\n",
    "sklearn's framework.\n",
    "\n",
    "Author: David S. Rosenberg <david.davidr@gmail.com>\n",
    "License: Creative Commons Attribution 4.0 International License\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import pandas as pd\n",
    "\n",
    "from setup_problem import load_problem\n",
    "\n",
    "class RidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" ridge regression\"\"\"\n",
    "\n",
    "    def __init__(self, l2reg=1):\n",
    "        if l2reg < 0:\n",
    "            raise ValueError('Regularization penalty should be at least 0.')\n",
    "        self.l2reg = l2reg\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n, num_ftrs = X.shape\n",
    "        # convert y to 1-dim array, in case we're given a column vector\n",
    "        y = y.reshape(-1)\n",
    "        def ridge_obj(w):\n",
    "            predictions = np.dot(X,w)\n",
    "            residual = y - predictions\n",
    "            empirical_risk = np.sum(residual**2) / n\n",
    "            l2_norm_squared = np.sum(w**2)\n",
    "            objective = empirical_risk + self.l2reg * l2_norm_squared\n",
    "            return objective\n",
    "        self.ridge_obj_ = ridge_obj\n",
    "\n",
    "        w_0 = np.zeros(num_ftrs)\n",
    "        self.w_ = minimize(ridge_obj, w_0).x\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        return np.dot(X, self.w_)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Average square error\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        residuals = self.predict(X) - y\n",
    "        return np.dot(residuals, residuals)/len(y)\n",
    "\n",
    "def compare_our_ridge_with_sklearn(X_train, y_train, l2_reg=1):\n",
    "    # First run sklearn ridge regression and extract the coefficients\n",
    "    from sklearn.linear_model import Ridge\n",
    "    # Fit with sklearn -- need to multiply l2_reg by sample size, since their\n",
    "    # objective function has the total square loss, rather than average square\n",
    "    # loss.\n",
    "    n = X_train.shape[0]\n",
    "    sklearn_ridge = Ridge(alpha=n*l2_reg, fit_intercept=False, normalize=False)\n",
    "    sklearn_ridge.fit(X_train, y_train)\n",
    "    sklearn_ridge_coefs = sklearn_ridge.coef_\n",
    "\n",
    "    # Now run our ridge regression and compare the coefficients to sklearn's\n",
    "    ridge_regression_estimator = RidgeRegression(l2reg=l2_reg)\n",
    "    ridge_regression_estimator.fit(X_train, y_train)\n",
    "    our_coefs = ridge_regression_estimator.w_\n",
    "\n",
    "    print(\"Hoping this is very close to 0:{}\".format(np.sum((our_coefs - sklearn_ridge_coefs)**2)))\n",
    "\n",
    "def do_grid_search_ridge(X_train, y_train, X_val, y_val):\n",
    "    # Now let's use sklearn to help us do hyperparameter tuning\n",
    "    # GridSearchCv.fit by default splits the data into training and\n",
    "    # validation itself; we want to use our own splits, so we need to stack our\n",
    "    # training and validation sets together, and supply an index\n",
    "    # (validation_fold) to specify which entries are train and which are\n",
    "    # validation.\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    val_fold = [-1]*len(X_train) + [0]*len(X_val) #0 corresponds to validation\n",
    "\n",
    "    # Now we set up and do the grid search over l2reg. The np.concatenate\n",
    "    # command illustrates my search for the best hyperparameter. In each line,\n",
    "    # I'm zooming in to a particular hyperparameter range that showed promise\n",
    "    # in the previous grid. This approach works reasonably well when\n",
    "    # performance is convex as a function of the hyperparameter, which it seems\n",
    "    # to be here.\n",
    "    \n",
    "    ### changing the grid\n",
    "    ###\n",
    "    ###\n",
    "    ### don't forget this has changed (note) \n",
    "    ###\n",
    "    ###\n",
    "    param_grid = [{'l2reg':np.unique(np.concatenate((10.**np.arange(-6,1,0.5),\n",
    "                                           np.arange(1,3,.3)\n",
    "                                             ))) }]\n",
    "\n",
    "    \n",
    "    \n",
    "    ridge_regression_estimator = RidgeRegression()\n",
    "    grid = GridSearchCV(ridge_regression_estimator,\n",
    "                        param_grid,\n",
    "                        return_train_score=True,\n",
    "                        cv = PredefinedSplit(test_fold=val_fold),\n",
    "                        refit = True,\n",
    "                        scoring = make_scorer(mean_squared_error,\n",
    "                                              greater_is_better = False))\n",
    "    grid.fit(X_train_val, y_train_val)\n",
    "\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    # Flip sign of score back, because GridSearchCV likes to maximize,\n",
    "    # so it flips the sign of the score if \"greater_is_better=FALSE\"\n",
    "    df['mean_test_score'] = -df['mean_test_score']\n",
    "    df['mean_train_score'] = -df['mean_train_score']\n",
    "    cols_to_keep = [\"param_l2reg\", \"mean_test_score\",\"mean_train_score\"]\n",
    "    df_toshow = df[cols_to_keep].fillna('-')\n",
    "    df_toshow = df_toshow.sort_values(by=[\"param_l2reg\"])\n",
    "    return grid, df_toshow\n",
    "\n",
    "def compare_parameter_vectors(pred_fns):\n",
    "    # Assumes pred_fns is a list of dicts, and each dict has a \"name\" key and a\n",
    "    # \"coefs\" key\n",
    "    fig, axs = plt.subplots(len(pred_fns),1, sharex=True, figsize = (20,20))\n",
    "    num_ftrs = len(pred_fns[0][\"coefs\"])\n",
    "    for i in range(len(pred_fns)):\n",
    "        title = pred_fns[i][\"name\"]\n",
    "        coef_vals = pred_fns[i][\"coefs\"]\n",
    "        axs[i].bar(range(num_ftrs), coef_vals, color='blue', linewidth=2, alpha=0.7)\n",
    "        axs[i].set_xlabel('Feature Index')\n",
    "        axs[i].set_ylabel('Parameter Value')\n",
    "        axs[i].set_title(title)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    return fig\n",
    "\n",
    "def plot_prediction_functions(x, pred_fns, x_train, y_train, legend_loc=\"best\"):\n",
    "    # Assumes pred_fns is a list of dicts, and each dict has a \"name\" key and a\n",
    "    # \"preds\" key. The value corresponding to the \"preds\" key is an array of\n",
    "    # predictions corresponding to the input vector x. x_train and y_train are\n",
    "    # the input and output values for the training data\n",
    "#     plt.figure(figsize=(20,10))\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.set_xlabel('Input Space: [0,1)')\n",
    "    ax.set_ylabel('Action/Outcome Space')\n",
    "    ax.set_title(\"Prediction Functions\")\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.scatter(x_train, y_train, label='Training data', color='blue', alpha=0.5, s=70)\n",
    "    for i in range(len(pred_fns)):\n",
    "        ax.plot(x, pred_fns[i][\"preds\"], label=pred_fns[i][\"name\"], linewidth=3.25, alpha=0.5)\n",
    "    legend = ax.legend(loc=legend_loc, shadow=True)\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    lasso_data_fname = \"lasso_data.pickle\"\n",
    "    x_train, y_train, x_val, y_val, target_fn, coefs_true, featurize = load_problem(lasso_data_fname)\n",
    "\n",
    "    # Generate features\n",
    "    X_train = featurize(x_train)\n",
    "    X_val = featurize(x_val)\n",
    "\n",
    "    #Visualize training data\n",
    "#     plt.figure(figsize=(20,10))\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    ax.imshow(X_train)\n",
    "    ax.set_title(\"Design Matrix: Color is Feature Value\")\n",
    "    ax.set_xlabel(\"Feature Index\")\n",
    "    ax.set_ylabel(\"Example Number\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "    # Compare our RidgeRegression to sklearn's.\n",
    "    compare_our_ridge_with_sklearn(X_train, y_train, l2_reg = 1.5)\n",
    "\n",
    "    # Do hyperparameter tuning with our ridge regression\n",
    "    grid, results = do_grid_search_ridge(X_train, y_train, X_val, y_val)\n",
    "    print(results)\n",
    "\n",
    "    # Plot validation performance vs regularization parameter ######################\n",
    "#     plt.figure(figsize=(20,10))\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "#    ax.loglog(results[\"param_l2reg\"], results[\"mean_test_score\"])\n",
    "    ax.semilogx(results[\"param_l2reg\"], results[\"mean_test_score\"],color='darkorange', linewidth=3.5, alpha = 0.5, marker='o')\n",
    "    ax.grid()\n",
    "    ax.set_title(\"Validation Performance vs L2 Regularization\")\n",
    "    ax.set_xlabel(\"L2-Penalty Regularization Parameter\")\n",
    "    ax.set_ylabel(\"Mean Squared Error\")\n",
    "    fig.show()\n",
    "\n",
    "    # Let's plot prediction functions and compare coefficients for several fits\n",
    "    # and the target function.\n",
    "    pred_fns = []\n",
    "    x = np.sort(np.concatenate([np.arange(0,1,.001), x_train]))\n",
    "    name = \"Target Parameter Values (i.e. Bayes Optimal)\"\n",
    "    pred_fns.append({\"name\":name, \"coefs\":coefs_true, \"preds\": target_fn(x) })\n",
    "\n",
    "    l2regs = [0, grid.best_params_['l2reg'], 1]\n",
    "    X = featurize(x)\n",
    "    for l2reg in l2regs:\n",
    "        ridge_regression_estimator = RidgeRegression(l2reg=l2reg)\n",
    "        ridge_regression_estimator.fit(X_train, y_train)\n",
    "        name = \"Ridge with L2Reg=\"+str(l2reg)\n",
    "        pred_fns.append({\"name\":name,\n",
    "                         \"coefs\":ridge_regression_estimator.w_,\n",
    "                         \"preds\": ridge_regression_estimator.predict(X) })\n",
    "\n",
    "    f = plot_prediction_functions(x, pred_fns, x_train, y_train, legend_loc=\"best\")\n",
    "    f.show()\n",
    "\n",
    "    f = compare_parameter_vectors(pred_fns)\n",
    "    f.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - RIDGE REGRESSION\n",
    "\n",
    "1. Run ridge regression on the provided training dataset. Choose the λ that minimizes the empirical risk (i.e. the average square loss) on the validation set. Include a table of the parameter values you tried and the validation performance for each. Also include a plot of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PREPARATION\n",
    "##\n",
    "## get_target_and_featurizer\n",
    "## you don't need to run this one again\n",
    "target_fn, coefs_true, featurize = get_target_and_featurizer(num_basis_fns = 100, num_nonzero = 10, coefs_true=None)\n",
    "##\n",
    "## generate_data\n",
    "##\n",
    "x, y = generate_data(target_fn, n=1000, noise_scale=.25, tdof=6)\n",
    "##\n",
    "## generate_problem\n",
    "##\n",
    "lasso_data_fname = \"lasso_data.pickle\"\n",
    "WRITE_PROBLEM=False\n",
    "n=1000\n",
    "test_frac=.6 ## CHANGED THIS ONE\n",
    "num_basis_fns=400\n",
    "num_nonzero=10\n",
    "noise_scale=.25 # scale factor on noise\n",
    "tdof = 6 # degrees of freedom of t-distribution generating noise\n",
    "x_train, y_train, x_val, y_val, target_fn, coefs_true, featurize = generate_problem(n=n, num_basis_fns=num_basis_fns, num_nonzero=num_nonzero, noise_scale=noise_scale, test_frac=test_frac, write_problem=WRITE_PROBLEM, file_name=lasso_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n",
      "(400,)\n",
      "(600, 400)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "x_train_featurized = featurize(x_train)\n",
    "x_val_featurized = featurize(x_val)\n",
    "\n",
    "print (x_train_featurized.shape)\n",
    "print (y_train.shape)\n",
    "print (x_val_featurized.shape)\n",
    "print (y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RidgeRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fb6d59f18f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## fitting the ridge regression on training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeRegression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RidgeRegression' is not defined"
     ]
    }
   ],
   "source": [
    "## fitting the ridge regression on training\n",
    "\n",
    "ridge = RidgeRegression.fit(RidgeRegression(), x_train_featurized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b4210940ea3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Checking predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ridge' is not defined"
     ]
    }
   ],
   "source": [
    "## Checking predict\n",
    "val_pred = ridge.predict(x_val_featurized, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cf8eb74f9414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Checking score: Mean Square Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ridge' is not defined"
     ]
    }
   ],
   "source": [
    "## Checking score: Mean Square Error\n",
    "ridge.score(x_val_featurized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_grid_search_ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-374c3163fc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdo_grid_search_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'do_grid_search_ridge' is not defined"
     ]
    }
   ],
   "source": [
    "## Grid Search for L2 Regularization Constant\n",
    "### changing the grid\n",
    "###\n",
    "###\n",
    "### don't forget that the function above has changed\n",
    "###\n",
    "###\n",
    "do_grid_search_ridge(x_train_featurized, y_train, x_val_featurized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_grid_search_ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7ae72cbf8501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgridsearch_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_grid_search_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'do_grid_search_ridge' is not defined"
     ]
    }
   ],
   "source": [
    "gridsearch_df = do_grid_search_ridge(x_train_featurized, y_train, x_val_featurized, y_val)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridsearch_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2438632bd972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgridsearch_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gridsearch_df' is not defined"
     ]
    }
   ],
   "source": [
    "gridsearch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridsearch_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-47f82fd36102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridsearch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_l2reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_error_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridsearch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_error_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridsearch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gridsearch_df' is not defined"
     ]
    }
   ],
   "source": [
    "lambdas = gridsearch_df['param_l2reg']\n",
    "training_error_mse = gridsearch_df['mean_train_score']\n",
    "test_error_mse = gridsearch_df['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_error_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training_error_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_error_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6af3f3b718f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'font.size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plt.plot(lambdas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_error_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Error vs. Validation Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_error_mse' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19793fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams['font.size'] = 16\n",
    "# plt.plot(lambdas)\n",
    "plt.plot(training_error_mse, linewidth=3.5, alpha = 0.5, marker='o')\n",
    "plt.plot(test_error_mse, linewidth=3.5, alpha = 0.5,  marker='o')\n",
    "plt.title('Training Error vs. Validation Error')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "# plt.savefig('ridge_training_validation_mse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we want to visualize the prediction functions. On the same axes, plot the following: the training data, the target function, an unregularized least squares fit (still using the featurized data), and the prediction function chosen in the previous problem. Next, along the lines of the bar charts produced by the code in compare_parameter_vectors, visualize the coefficients for each of the prediction functions plotted, including the target function. Describe the patterns, including the scale of the coefficients, as well as which coefficients have the most weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_grid_search_ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d2e1bbfd62a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_fns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcoefs_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_grid_search_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ml2regs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'do_grid_search_ridge' is not defined"
     ]
    }
   ],
   "source": [
    "pred_fns = []\n",
    "x = np.sort(np.concatenate([np.arange(0,1,.001), x_train]))\n",
    "name = \"Target Parameter Values (i.e. Bayes Optimal)\"\n",
    "pred_fns.append({\"name\":name, \"coefs\":coefs_true, \"preds\": target_fn(x) })\n",
    "\n",
    "grid = do_grid_search_ridge(x_train_featurized, y_train, x_val_featurized, y_val)[0]\n",
    "l2regs = [0, grid.best_params_['l2reg'], 1]\n",
    "X = featurize(x)\n",
    "\n",
    "for l2reg in l2regs:\n",
    "    ridge_regression_estimator = RidgeRegression(l2reg=l2reg)\n",
    "    ridge_regression_estimator.fit(x_train_featurized, y_train)\n",
    "    name = \"Ridge with L2Reg=\"+str(l2reg)\n",
    "    pred_fns.append({\"name\":name,\n",
    "                         \"coefs\":ridge_regression_estimator.w_,\n",
    "                         \"preds\": ridge_regression_estimator.predict(X) })\n",
    "\n",
    "plot_prediction_functions(x, pred_fns, x_train, y_train, legend_loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. For the chosen λ, examine the model coefficients. For ridge regression, we don’t expect any parameters to be exactly 0. However, let’s investigate whether we can predict the sparsity pattern of the true parameters (i.e. which parameters are 0 and which are nonzero) by thresholding the parameter estimates we get from ridge regression. We’ll predict that wi = 0 if |wˆi| < ε and wi ̸= 0 otherwise. Give the confusion matrix for ε = 10−6, 10−3, 10−1, and any other thresholds you would like to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-af7c9de940bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m \u001b[0;31m##chosen lambda_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "grid.best_params_ ##chosen lambda_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RidgeRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bbd06bac995f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mridge_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l2reg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RidgeRegression' is not defined"
     ]
    }
   ],
   "source": [
    "ridge_best = RidgeRegression(l2reg=grid.best_params_['l2reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bed415dbde85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mridge_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ridge_best' is not defined"
     ]
    }
   ],
   "source": [
    "ridge_best.fit(x_train_featurized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'coefs', 'preds'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fns[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-22-09b043a07544>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-09b043a07544>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    #     print (pred_fns[i]['coefs'])\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred_fns)):\n",
    "#     print (pred_fns[i]['name'])\n",
    "#     print (pred_fns[i]['coefs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-569ff1835a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambda_best_coefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coefs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# lambda_best_coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lambda_best_coefs = pred_fns[2]['coefs']\n",
    "# lambda_best_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilons = [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e02851da96a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepsilon_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mepsilon_sparse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_best_coefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilons' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "epsilon_sparse = {}\n",
    "\n",
    "for epsilon in epsilons: \n",
    "    epsilon_sparse[epsilon] = np.array(pd.Series(lambda_best_coefs).apply(lambda x: int(np.absolute(x) > epsilon)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual_sparse = {}\n",
    "\n",
    "for key in epsilon_sparse.keys():\n",
    "    actual_sparse[key] = epsilon_sparse[key]*lambda_best_coefs\n",
    "\n",
    "# actual_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-77a80777c2ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_trues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_best_coefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_sparse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilons' is not defined"
     ]
    }
   ],
   "source": [
    "y_trues = {}\n",
    "y_preds = {}\n",
    "conf_ms = {}\n",
    "\n",
    "for epsilon in epsilons: \n",
    "    y_trues[epsilon] = np.array(pd.Series(lambda_best_coefs).apply(lambda x: int(x > 0)))\n",
    "    y_preds[epsilon] = np.array(pd.Series(actual_sparse[epsilon]).apply(lambda x: int(x > 0)))\n",
    "#     conf_matrix = confusion_matrix(lambda_best_coefs, actual_sparse[epsilon])\n",
    "    conf_ms[epsilon] = confusion_matrix(y_trues[epsilon], y_preds[epsilon])\n",
    "#     plt.imshow(conf_ms[epsilon])\n",
    "#    print ('epsilon = ' + str(epsilon) + '\\n')\n",
    "#    print (str(conf_ms[epsilon]) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - COORDINATE DESCENT FOR LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The algorithm as described above is not ready for a large dataset (at least if it has being im- plemented in Python) because of the implied loop in the summation signs for the expressions for aj and cj. Give an expression for computing aj and cj using matrix and vector operations, without explicit loops. This is called “vectorization” and can lead to dramatic speedup when implemented in languages such as Python, Matlab, and R. Write your expressions using X, w, y = (y1, . . . , yn)T (the column vector of responses), X·j (the jth column of X, represented as a column matrix), and wj (the jth coordinate of w – a scalar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__a_j__ = (2*np.dot(np.transpose(X[:, i]), X[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss ### np.sum(theta oldugundan emin miyiz)\n",
    "    loss =+ np.sum(np.square(np.dot(X, theta) - y)) + lambda_reg*np.sum(theta) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_thresholding(p, r):\n",
    "    if p > r:\n",
    "        return p-r\n",
    "    elif np.absolute(p) <= r:\n",
    "        return 0\n",
    "    elif p < (-1*r):\n",
    "        return p+r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def shooting(X, y, theta, method='randomized', theta_init='zeros', max_iter=1000, conv_threshold=1e-8):\n",
    "    \n",
    "#     a = np.zeros(len(theta))  ## bunlari 0'da initialize etmek iyi bir fikir mi???\n",
    "#     c = np.zeros(len(theta))\n",
    "    \n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((max_iter, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(max_iter) #Initialize loss_hist\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while i in range(max_iter):\n",
    "        \n",
    "        \n",
    "    \n",
    "            if theta_init == 'zeros':\n",
    "            \n",
    "                theta = np.zeros(num_features) ## bu ayni zamanda lambda = lambda_max oldugunu imply ediyor, bunu nasil uygulayacagiz?? \n",
    "\n",
    "                lambda_reg = 2*np.max(np.dot(np.transpose(X), y))\n",
    "                \n",
    "                a = np.zeros((max_iter, len(theta)))  ## bunlari 0'da initialize etmek iyi bir fikir mi???\n",
    "                c = np.zeros((max_iter, len(theta)))\n",
    "\n",
    "                if method == 'randomized':\n",
    "                    \n",
    "                    loss = compute_regularized_square_loss(X, y, theta, lambda_reg)\n",
    "                    loss_hist[i] = loss\n",
    "                    \n",
    "                    while loss_hist[i] > conv_threshold:\n",
    "\n",
    "#                         np.random.shuffle(theta[inside_iter])\n",
    "\n",
    "                        theta_hist[i] = theta\n",
    "\n",
    "                        j = np.random.choice(len(theta))\n",
    "\n",
    "                        print ('j ' + '=' + str(j))\n",
    "\n",
    "                        ### a_j_vectorized\n",
    "                        a[i][j] = 2*(np.dot(np.transpose(X[:,j]), X[:,j]))\n",
    "\n",
    "                        print (a)\n",
    "                        ### c_j vectorized\n",
    "                        c[i][j] = 2*((np.dot(X[:,j], y)) -(np.dot(np.dot(X, theta), X[:,j])) + (np.dot(np.dot(theta[j], X[:,j]), X[:,j])))\n",
    "\n",
    "                        print (c)\n",
    "                        ### updating theta_j\n",
    "                        m = c[i][j]/a[i][j]\n",
    "\n",
    "                        print (m)\n",
    "\n",
    "                        delta = (lambda_reg/a[i][j])\n",
    "\n",
    "                        print (delta)\n",
    "\n",
    "    #                     theta[j] = soft_thresholding(m, delta)   ### soft_function, dogru mu google it\n",
    "\n",
    "                        if m > delta:\n",
    "                            theta[j] = m-delta\n",
    "                        elif np.absolute(m) <= delta:\n",
    "                            theta[j] = 0\n",
    "                        elif m < (-1*delta):\n",
    "                            theta[j] = m+delta\n",
    "\n",
    "                        print ('theta[j]' + str(theta[j]))\n",
    "\n",
    "                        theta_hist[i] = theta\n",
    "                                ## loss'u tekrar hesaplamadan theta'yi update ediyor olduguna emin ol \n",
    "                        \n",
    "\n",
    "                        print (loss)\n",
    "\n",
    "                \n",
    "\n",
    "#                     while loss_hist[i][inside_iter][-1] - loss_hist[i][inside_iter-1][-1] <= conv_threshold:\n",
    "#                         break\n",
    "                        \n",
    "                    \n",
    "\n",
    "                elif method == 'cyclic':\n",
    "\n",
    "\n",
    "                    for j in range(len(theta)):\n",
    "                        \n",
    "                         ### a_j_vectorized\n",
    "                        a[j] = 2*(np.dot(np.transpose(X[:,j]), X[:,j]))\n",
    "                        ### c_j vectorized\n",
    "                        c[j] = 2*((np.dot(X[:,j], y)) -(np.dot(np.dot(X, theta), X[:,j])) + (np.dot(np.dot(theta[j], X[:,j]), X[:,j])))\n",
    "                        ### updating theta_j\n",
    "                        m = c[j]/a[j]\n",
    "                        delta = (lambda_reg/a[j])\n",
    "                        \n",
    "                        theta[i][j] = soft_thresholding(m, delta)   ### soft_function, dogru mu google it\n",
    "                        \n",
    "                        ## loss'u tekrar hesaplamadan theta'yi update ediyor olduguna emin ol \n",
    "                        loss = compute_regularized_square_loss(X, y, theta[i][j], lambda_reg)\n",
    "                        loss_hist[i][j] = loss\n",
    "                        \n",
    "                    while loss_hist[i] <= conv_threshold:\n",
    "                        break\n",
    "\n",
    "            elif theta_init == 'warm_start':\n",
    "                \n",
    "                ## digerinin best cozumu ile basliyoruz, bastan 0la degil\n",
    "\n",
    "\n",
    "                if method == 'randomized':\n",
    "\n",
    "                    np.random.shuffle(theta)\n",
    "                    \n",
    "                    for j in range(len(theta)):\n",
    "                        a[j] = 2*(np.dot(np.transpose(X[:,j]), X[:,j]))\n",
    "                        ## loss'u tekrar hesaplamadan theta'yi update ediyor olduguna emin ol \n",
    "                        loss = compute_regularized_square_loss(X, y, theta, lambda_reg)\n",
    "                        loss_hist[i] = loss\n",
    "\n",
    "                elif method == 'cyclic':\n",
    "\n",
    "\n",
    "                    for j in range(len(theta)):\n",
    "                        a[j] = 2*(np.dot(np.transpose(X[:,j]), X[:,j]))\n",
    "                        ## loss'u tekrar hesaplamadan theta'yi update ediyor olduguna emin ol \n",
    "                        loss = compute_regularized_square_loss(X, y, theta, lambda_reg)\n",
    "                        loss_hist[i] = loss\n",
    "\n",
    "                    \n",
    "#             while loss_hist[i][-1] - loss_hist[i - 1][-1] > conv_threshold:\n",
    "            i = i + 1 ### bu dogru yerde mi yoksa convergence threshold'un altinda mi olmali bilmiyorum\n",
    "        ## her loopun sonuna koyabiliriz\n",
    "    return theta_hist, loss_hist\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shooting(x_train_featurized, y_train, theta, method='randomized', theta_init='zeros', max_iter=1000, conv_threshold=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
